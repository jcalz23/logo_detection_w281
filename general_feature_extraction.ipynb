{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19368,"status":"ok","timestamp":1670542296131,"user":{"displayName":"John Calzaretta","userId":"15052052157149365905"},"user_tz":420},"id":"dZFDBOBnMTX6","outputId":"b6051605-491c-42d5-f7a5-0697d2e7395a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Requirement already satisfied: scikit-image in /usr/local/lib/python3.8/dist-packages (0.18.3)\n","Collecting scikit-image\n","  Downloading scikit_image-0.19.3-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (14.0 MB)\n","\u001b[K     |████████████████████████████████| 14.0 MB 5.2 MB/s \n","\u001b[?25hRequirement already satisfied: numpy>=1.17.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.21.6)\n","Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.7.3)\n","Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (7.1.2)\n","Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (2.8.8)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (21.3)\n","Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (1.4.1)\n","Requirement already satisfied: imageio>=2.4.1 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (2.9.0)\n","Requirement already satisfied: tifffile>=2019.7.26 in /usr/local/lib/python3.8/dist-packages (from scikit-image) (2022.10.10)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->scikit-image) (3.0.9)\n","Installing collected packages: scikit-image\n","  Attempting uninstall: scikit-image\n","    Found existing installation: scikit-image 0.18.3\n","    Uninstalling scikit-image-0.18.3:\n","      Successfully uninstalled scikit-image-0.18.3\n","Successfully installed scikit-image-0.19.3\n"]}],"source":["!pip install -U scikit-image # this needs to be installed with each new runtime on colab"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":19673,"status":"ok","timestamp":1670542315798,"user":{"displayName":"John Calzaretta","userId":"15052052157149365905"},"user_tz":420},"id":"dpVH3xT-U7az","outputId":"269d7205-20a5-4faa-ef40-cb9f08d62502"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Mount Drive to Access Data\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"23DUR5dwbfVM","executionInfo":{"status":"ok","timestamp":1670542699237,"user_tz":420,"elapsed":383444,"user":{"displayName":"John Calzaretta","userId":"15052052157149365905"}}},"outputs":[],"source":["# Authenticate to access cloud bucket\n","from google.colab import auth\n","auth.authenticate_user()\n","\n","# create a LOCAL directory in /content/  so you can move stuff from bucket to local\n","!mkdir /content/w281FinalProjectLogo\n","# copy from google bucket to local directory\n","!gsutil -m -q cp -r gs://w281finalprojectlogo/Logos-32plus_v1.0.1/preprocessedv4 /content/w281FinalProjectLogo/"]},{"cell_type":"markdown","metadata":{"id":"3uY-Z4Y0U6hK"},"source":["## 1. Imports and Paths"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"N725zelSVIob","executionInfo":{"status":"ok","timestamp":1670542699238,"user_tz":420,"elapsed":12,"user":{"displayName":"John Calzaretta","userId":"15052052157149365905"}}},"outputs":[],"source":["# Define directories\n","base_dir = '/content/drive/MyDrive/w281FinalProjectLogo/'\n","bucket = '/content/w281FinalProjectLogo/'\n","fe_dir = base_dir + '/utilities/'\n","drive_save_dir = base_dir + 'Logos-32plus_v1.0.1/feature_extraction/'\n","\n","# reading, writing to bucket\n","preproc_dir = bucket + 'preprocessedv4/'\n","bucket_save_dir = bucket + 'feature_extraction/'\n","da_path = preproc_dir + 'da/'\n","bb_path = preproc_dir + 'bb/'\n","cn_path = preproc_dir + 'cn/'"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"x-_4LmclU6hO","executionInfo":{"status":"ok","timestamp":1670542702319,"user_tz":420,"elapsed":3091,"user":{"displayName":"John Calzaretta","userId":"15052052157149365905"}}},"outputs":[],"source":["# Playing with labeled image data\n","import os\n","import sys\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import pickle\n","import cv2\n","import json\n","from tqdm import tqdm\n","import skimage.feature as feat\n","\n","sys.path.append(fe_dir)\n","from fe_helper_functions import HarrisKeypointDetector, SimpleFeatureDescriptor, \\\n","    ORB_SIFT_FeatureDescriptor, extract_color_moments, hu_moments\n","\n","try:\n","  os.mkdir(bucket_save_dir)\n","except:\n","  pass"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"MOPsHgix1EYZ","executionInfo":{"status":"ok","timestamp":1670542702320,"user_tz":420,"elapsed":13,"user":{"displayName":"John Calzaretta","userId":"15052052157149365905"}}},"outputs":[],"source":["# Set script params\n","new_df = True\n","resume_fe = False\n","\n","# Set texture feature params\n","distances = [1, 2]\n","angles = [0, np.pi/8, np.pi/4, 3*np.pi/8, np.pi/2,\n","          5*np.pi/8, 3*np.pi/4, 7*np.pi/8]"]},{"cell_type":"markdown","metadata":{"id":"xwnj1eeAU6hQ"},"source":["## 2. Ingest Split BBoxes\n","Goal: For each split, get a list of image paths that we can load and loop through later"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2803,"status":"ok","timestamp":1670543223582,"user":{"displayName":"John Calzaretta","userId":"15052052157149365905"},"user_tz":420},"id":"O-C-Mj23U6hR","outputId":"bf109ccf-6707-485d-df6f-9d5d7ed969d2"},"outputs":[{"output_type":"stream","name":"stdout","text":["Val cn_bb count: 674\n","Test cn_bb count: 677\n","Train da_bb count: 6010\n","Loop df rows: 7361\n"]}],"source":["if new_df or resume_fe:\n","\n","  # Step 1: Get the map of base images to bb files w/ split info\n","  bb_map_file = preproc_dir + 'preproc_map_cn.json'\n","  bb_map = pd.read_json(bb_map_file).T\n","  bb_map.index = bb_map.index.set_names(['img_path'])\n","  bb_map.reset_index(level=0, inplace=True)\n","  bb_map['image_name'] = bb_map['image_source'].apply(lambda x: x.split('/')[-1])\n","\n","  # Step 2: get bb image list for val and test images\n","  val_df = bb_map.loc[bb_map['set']=='val', ].copy()\n","  test_df = bb_map.loc[bb_map['set']=='test', ].copy()\n","\n","  # Step 3: get da image list for train images\n","  da_map_file = preproc_dir + 'preproc_map_da.json'\n","  da_map = pd.read_json(da_map_file).T\n","  da_map.index = da_map.index.set_names(['img_path'])\n","  da_map.reset_index(level=0, inplace=True)\n","  da_map['image_name'] = da_map['image_source'].apply(lambda x: x.split('/')[-1])\n","  train_df = da_map.loc[da_map['set']=='train', ].copy()\n","\n","  # Step 4: Combine the train, test, val dfs to extract features in same loop\n","  loop_df = pd.concat([train_df, val_df, test_df], axis=0)\n","  loop_df.reset_index(inplace=True, drop=True)\n","\n","  # Get counts\n","  print(f\"Val cn_bb count: {len(val_df)}\")\n","  print(f\"Test cn_bb count: {len(test_df)}\")\n","  print(f\"Train da_bb count: {len(train_df)}\")\n","  print(f\"Loop df rows: {len(loop_df)}\")"]},{"cell_type":"markdown","metadata":{"id":"fxw0OSoMU6hT"},"source":["## 3. Extract Features\n","Plan: Loop through each train, val, test list and extract features"]},{"cell_type":"code","execution_count":12,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2,"status":"ok","timestamp":1670543223745,"user":{"displayName":"John Calzaretta","userId":"15052052157149365905"},"user_tz":420},"id":"CTGZEvhTGsCl","outputId":"e140ac4c-012a-4f8e-e6c5-168bab00c470"},"outputs":[{"output_type":"stream","name":"stdout","text":["Initialize new feature df\n"]}],"source":["if new_df and not resume_fe:\n","  ## Initialize empty df for extracted features\n","  features = pd.DataFrame({\n","    'image_path': pd.Series(dtype='str'),\n","    'image_name': pd.Series(dtype='str'),\n","    'class': pd.Series(dtype='str'),\n","    'class_code':  pd.Series(dtype='int'),\n","    'split': pd.Series(dtype='str'),\n","    })\n","  print(\"Initialize new feature df\")\n","elif resume_fe:\n","  # Load df that has been processed, find next episodes to work with\n","  features = pd.read_pickle(drive_save_dir+'fe_subset_120822.csv')\n","  loop_df = loop_df.loc[~loop_df['img_path'].isin(features['image_path']), ].copy() # subset to unprocessed records\n","  loop_df.reset_index(inplace=True, drop=True)\n","  print(f\"Remaining records to process: {len(loop_df)}\")\n","else:\n","  # Load df that has been processed\n","  features = pd.read_pickle(drive_save_dir+'fe_subset_120822.csv')\n","  print(f\"Loaded complete feature df w/ {len(features)} rows\")"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e-lJM6nKU6hV","executionInfo":{"status":"ok","timestamp":1670554448840,"user_tz":420,"elapsed":11221108,"user":{"displayName":"John Calzaretta","userId":"15052052157149365905"}},"outputId":"9c07b512-a6eb-4d99-965b-19b9bbe55754"},"outputs":[{"output_type":"stream","name":"stderr","text":[" 14%|█▎        | 999/7361 [32:07<47:44,  2.22it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Saving df with 1000 records\n"]},{"output_type":"stream","name":"stderr","text":[" 27%|██▋       | 1999/7361 [45:18<1:09:58,  1.28it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Saving df with 2000 records\n"]},{"output_type":"stream","name":"stderr","text":[" 41%|████      | 2999/7361 [1:09:09<1:35:17,  1.31s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Saving df with 3000 records\n"]},{"output_type":"stream","name":"stderr","text":[" 54%|█████▍    | 3999/7361 [1:39:01<3:39:08,  3.91s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Saving df with 4000 records\n"]},{"output_type":"stream","name":"stderr","text":[" 68%|██████▊   | 4999/7361 [2:17:36<24:01,  1.64it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Saving df with 5000 records\n"]},{"output_type":"stream","name":"stderr","text":[" 81%|████████▏ | 5999/7361 [2:33:24<09:01,  2.52it/s]"]},{"output_type":"stream","name":"stdout","text":["\n","Saving df with 6000 records\n"]},{"output_type":"stream","name":"stderr","text":[" 95%|█████████▌| 6999/7361 [2:59:08<07:46,  1.29s/it]"]},{"output_type":"stream","name":"stdout","text":["\n","Saving df with 7000 records\n"]},{"output_type":"stream","name":"stderr","text":["100%|██████████| 7361/7361 [3:06:59<00:00,  1.52s/it]\n"]},{"output_type":"stream","name":"stdout","text":["Total FE errors: 0\n"]}],"source":["## Train Loop\n","skipped = 0\n","if new_df | resume_fe:\n","  for idx, row in tqdm(loop_df.iterrows(), total=loop_df.shape[0]):\n","    try:\n","      # Step 1: load rgb images, create gray, hsv, ycrcb copies\n","      im_path = da_path if row['set']=='train' else cn_path\n","      bbox_rgb = plt.imread(im_path + row['img_path']) # read img\n","      bbox_gray = cv2.cvtColor(bbox_rgb, cv2.COLOR_RGB2GRAY) # convert to gray\n","      bbox_hsv = cv2.cvtColor(bbox_rgb, cv2.COLOR_RGB2HSV) # convert to HSV\n","      bbox_ycrcb = cv2.cvtColor(bbox_rgb, cv2.COLOR_RGB2YCR_CB) # convert to YCRCB\n","\n","      # Step 2: extract moment features\n","      cm_rgb_mean, cm_rgb_var, cm_rgb_skew = extract_color_moments(bbox_rgb)\n","      cm_hsv_mean, cm_hsv_var, cm_hsv_skew = extract_color_moments(bbox_hsv)\n","      cm_ycrcb_mean, cm_ycrcb_var, cm_ycrcb_skew = extract_color_moments(bbox_ycrcb)\n","      hu_mom = hu_moments(bbox_gray).reshape(-1)\n","\n","      # Step 3: Get texture features\n","      graycom = feat.graycomatrix(bbox_gray, distances, angles, levels=256)\n","      contrast = feat.graycoprops(graycom, 'contrast').reshape(-1)\n","      dissimilarity = feat.graycoprops(graycom, 'dissimilarity').reshape(-1)\n","      homogeneity = feat.graycoprops(graycom, 'homogeneity').reshape(-1)\n","      energy = feat.graycoprops(graycom, 'energy').reshape(-1)\n","      correlation = feat.graycoprops(graycom, 'correlation').reshape(-1)\n","      ASM = feat.graycoprops(graycom, 'ASM').reshape(-1)\n","\n","      # Append new cols to df\n","      row_features = {\n","          # info from preproc_map\n","          'image_path': row['img_path'],\n","          'image_name': row['image_name'],\n","          'class': row['class'],\n","          'class_code':  row['class_code'],\n","          'split': row['set'],\n","          # moment features\n","          'hu_moments': hu_mom,\n","          'cm_rgb_mean': cm_rgb_mean,\n","          'cm_rgb_var': cm_rgb_var,\n","          'cm_rgb_skew': cm_rgb_skew,\n","          'cm_hsv_mean': cm_hsv_mean,\n","          'cm_hsv_var': cm_hsv_var,\n","          'cm_hsv_skew': cm_hsv_skew,\n","          'cm_ycrcb_mean': cm_ycrcb_mean,\n","          'cm_ycrcb_var': cm_ycrcb_var,\n","          'cm_ycrcb_skew': cm_ycrcb_skew,\n","          # texture features\n","          'contrast': contrast,\n","          'dissimilarity': dissimilarity,\n","          'homogeneity': homogeneity,\n","          'energy': energy,\n","          'correlation': correlation,\n","          'ASM': ASM,\n","          }\n","      features = features.append(row_features, ignore_index=True)\n","\n","      if ((idx+1)%1000==0): # save every 1k records out\n","        print(f\"\\nSaving df with {len(features)} records\")\n","        features.to_pickle(drive_save_dir+'fe_subset_120822.csv')\n","\n","    except:\n","      skipped += 1\n","      print(f\"Error #{skipped}\")\n","      pass\n","\n","# Save final df\n","features.to_pickle(drive_save_dir+'fe_subset_120822.csv')\n","print(f\"Total FE errors: {skipped}\")"]},{"cell_type":"markdown","metadata":{"id":"enlFvlEe1owN"},"source":["## Merge in SIFT Histogram Features"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"uysrT97KeMEM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1670554471712,"user_tz":420,"elapsed":22878,"user":{"displayName":"John Calzaretta","userId":"15052052157149365905"}},"outputId":"ebdf345c-77e9-4689-a888-121e078c2dfe"},"outputs":[{"output_type":"stream","name":"stdout","text":["7361\n","7361\n","Null SIFT feature count: 2\n"]}],"source":["# Load SIFT\n","sift_train = pd.read_pickle(drive_save_dir+'train_set.pkl')\n","sift_val = pd.read_pickle(drive_save_dir+'val_set.pkl')\n","sift_test = pd.read_pickle(drive_save_dir+'test_set.pkl')\n","sift = pd.concat([sift_train, sift_val, sift_test], axis=0)\n","\n","# Merge\n","sift['merge_key'] = sift['file'].apply(lambda x: '_'.join(x[:-4].split(\"_\")[:6]))\n","features['merge_key'] = features['image_path'].apply(lambda x: '_'.join(x[:-4].split(\"_\")[:6]))\n","print(len(features))\n","features = pd.merge(features, sift[['merge_key', 'norm_hist']], on='merge_key', how='inner')\n","print(len(features))\n","\n","num_null = len(features.loc[features['norm_hist'].isnull(), ])\n","print(f\"Null SIFT feature count: {num_null}\")\n","\n","# Save out merged features\n","features.to_pickle(drive_save_dir+'fe_merged_120822.csv')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o8fM4fQ6kpRq"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3.10.0 ('w281')","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.0"},"vscode":{"interpreter":{"hash":"7690e95adedd8191f0ce86c5df02585ffc130729bec63a8b5bda66e6900601ad"}}},"nbformat":4,"nbformat_minor":0}