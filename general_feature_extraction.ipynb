{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install -U scikit-image"
      ],
      "metadata": {
        "id": "dZFDBOBnMTX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpVH3xT-U7az",
        "outputId": "b0776160-871c-46b0-d880-fbfa986f67ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Drive to Access Data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23DUR5dwbfVM"
      },
      "outputs": [],
      "source": [
        "# Authenticate to access cloud bucket\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# create a LOCAL directory in /content/  so you can move stuff from bucket to local\n",
        "!mkdir /content/w281FinalProjectLogo\n",
        "!mkdir /content/w281FinalProjectLogo/Logos-32plus_v1.0.1\n",
        "# copy from google bucket to local directory\n",
        "!gsutil -m -q cp -r gs://w281finalprojectlogo/Logos-32plus_v1.0.1 /content/w281FinalProjectLogo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3uY-Z4Y0U6hK"
      },
      "source": [
        "## 1. Imports and Paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N725zelSVIob"
      },
      "outputs": [],
      "source": [
        "# Define directories\n",
        "base_dir = '/content/drive/MyDrive/w281FinalProjectLogo/'\n",
        "bucket = '/content/w281FinalProjectLogo/Logos-32plus_v1.0.1/'\n",
        "john_dir = base_dir + '/john/'\n",
        "fe_dir = base_dir + '/john/feature_extraction/'\n",
        "drive_save_dir = base_dir + 'Logos-32plus_v1.0.1/feature_extraction/'\n",
        "\n",
        "# reading, writing to bucket\n",
        "preproc_dir = bucket + 'preprocessed/'\n",
        "bucket_save_dir = bucket + 'feature_extraction/'\n",
        "da_path = preproc_dir + 'da/'\n",
        "bb_path = preproc_dir + 'bb/'\n",
        "cn_path = preproc_dir + 'cn/'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "x-_4LmclU6hO"
      },
      "outputs": [],
      "source": [
        "# Playing with labeled image data\n",
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import pickle\n",
        "import cv2\n",
        "import json\n",
        "from tqdm import tqdm\n",
        "import skimage.feature as feat\n",
        "\n",
        "sys.path.append(fe_dir)\n",
        "from helper_functions import HarrisKeypointDetector, SimpleFeatureDescriptor, \\\n",
        "    ORB_SIFT_FeatureDescriptor, extract_color_moments, hu_moments\n",
        "\n",
        "try:\n",
        "  os.mkdir(bucket_save_dir)\n",
        "except:\n",
        "  pass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOPsHgix1EYZ"
      },
      "outputs": [],
      "source": [
        "# Set script params\n",
        "new_df = False\n",
        "resume_fe = True\n",
        "\n",
        "# Set texture feature params\n",
        "distances = [1, 2]\n",
        "angles = [0, np.pi/8, np.pi/4, 3*np.pi/8, np.pi/2,\n",
        "          5*np.pi/8, 3*np.pi/4, 7*np.pi/8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xwnj1eeAU6hQ"
      },
      "source": [
        "## 2. Ingest Split BBoxes\n",
        "Goal: For each split, get a list of image paths that we can load and loop through later"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "O-C-Mj23U6hR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "991cd3dd-871f-4d94-b94e-2bf56a432e31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Val bb count: 361\n",
            "Test bb count: 434\n",
            "Train da_bb count: 10789\n",
            "Loop df rows: 11584\n"
          ]
        }
      ],
      "source": [
        "if new_df or resume_fe:\n",
        "\n",
        "  # Step 1: Get the map of base images to bb files w/ split info\n",
        "  bb_map_file = preproc_dir + 'preproc_map_cn.json'\n",
        "  bb_map = pd.read_json(bb_map_file).T\n",
        "  bb_map.index = bb_map.index.set_names(['img_path'])\n",
        "  bb_map.reset_index(level=0, inplace=True)\n",
        "  bb_map['image_name'] = bb_map['image_source'].apply(lambda x: x.split('/')[-1])\n",
        "\n",
        "  # Step 2: get bb image list for val and test images\n",
        "  val_df = bb_map.loc[bb_map['set']=='val', ].copy()\n",
        "  test_df = bb_map.loc[bb_map['set']=='test', ].copy()\n",
        "\n",
        "  # Step 3: get da image list for train images\n",
        "  da_map_file = preproc_dir + 'preproc_map_da.json'\n",
        "  da_map = pd.read_json(da_map_file).T\n",
        "  da_map.index = da_map.index.set_names(['img_path'])\n",
        "  da_map.reset_index(level=0, inplace=True)\n",
        "  da_map['image_name'] = da_map['image_source'].apply(lambda x: x.split('/')[-1])\n",
        "  train_df = da_map.loc[da_map['set']=='train', ].copy()\n",
        "\n",
        "  # Step 4: Combine the train, test, val dfs to extract features in same loop\n",
        "  loop_df = pd.concat([train_df, val_df, test_df], axis=0)\n",
        "  loop_df.reset_index(inplace=True, drop=True)\n",
        "\n",
        "  # Get counts\n",
        "  print(f\"Val bb count: {len(val_df)}\")\n",
        "  print(f\"Test bb count: {len(test_df)}\")\n",
        "  print(f\"Train da_bb count: {len(train_df)}\")\n",
        "  print(f\"Loop df rows: {len(loop_df)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxw0OSoMU6hT"
      },
      "source": [
        "## 3. Extract Features\n",
        "Plan: Loop through each train, val, test list and extract features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CTGZEvhTGsCl",
        "outputId": "437b0da6-59e3-4b7e-b8f2-83dcc35aafc5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Remaining records to process: 795\n"
          ]
        }
      ],
      "source": [
        "if new_df and not resume_fe:\n",
        "  ## Initialize empty df for extracted features\n",
        "  features = pd.DataFrame({\n",
        "    'image_path': pd.Series(dtype='str'),\n",
        "    'image_source': pd.Series(dtype='str'),\n",
        "    'bbox_source': pd.Series(dtype='str'),\n",
        "    'class': pd.Series(dtype='str'),\n",
        "    'class_code':  pd.Series(dtype='int'),\n",
        "    'split': pd.Series(dtype='str'),\n",
        "    })\n",
        "  print(\"Initialize new feature df\")\n",
        "elif resume_fe:\n",
        "  # Load df that has been processed, find next episodes to work with\n",
        "  features = pd.read_pickle(drive_save_dir+'fe_subset_112722.csv')\n",
        "  loop_df = loop_df.loc[~loop_df['img_path'].isin(features['image_path']), ].copy() # subset to unprocessed records\n",
        "  loop_df.reset_index(inplace=True, drop=True)\n",
        "  print(f\"Remaining records to process: {len(loop_df)}\")\n",
        "else:\n",
        "  # Load df that has been processed\n",
        "  features = pd.read_pickle(drive_save_dir+'fe_subset_112722.csv')\n",
        "  print(f\"Loaded complete feature df w/ {len(features)} rows\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e-lJM6nKU6hV",
        "outputId": "88322235-1609-4bed-c920-c9860a9ee6c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 795/795 [16:45<00:00,  1.26s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total FE errors: 0\n"
          ]
        }
      ],
      "source": [
        "## Train Loop\n",
        "skipped = 0\n",
        "if new_df | resume_fe:\n",
        "  for idx, row in tqdm(loop_df.iterrows(), total=loop_df.shape[0]):\n",
        "    try:\n",
        "      # Step 1: load rgb images, create gray, hsv, ycrcb copies\n",
        "      im_path = da_path if row['set']=='train' else cn_path\n",
        "      bbox_rgb = plt.imread(im_path + row['img_path']) # read img\n",
        "      bbox_gray = cv2.cvtColor(bbox_rgb, cv2.COLOR_RGB2GRAY) # convert to gray\n",
        "      bbox_hsv = cv2.cvtColor(bbox_rgb, cv2.COLOR_RGB2HSV) # convert to HSV\n",
        "      bbox_ycrcb = cv2.cvtColor(bbox_rgb, cv2.COLOR_RGB2YCR_CB) # convert to YCRCB\n",
        "\n",
        "      # Step 2: extract moment features\n",
        "      cm_rgb_mean, cm_rgb_var, cm_rgb_skew = extract_color_moments(bbox_rgb)\n",
        "      cm_hsv_mean, cm_hsv_var, cm_hsv_skew = extract_color_moments(bbox_hsv)\n",
        "      cm_ycrcb_mean, cm_ycrcb_var, cm_ycrcb_skew = extract_color_moments(bbox_ycrcb)\n",
        "      hu_mom = hu_moments(bbox_gray).reshape(-1)\n",
        "\n",
        "      # Step 3: Get texture features\n",
        "      graycom = feat.graycomatrix(bbox_gray, distances, angles, levels=256)\n",
        "      contrast = feat.graycoprops(graycom, 'contrast').reshape(-1)\n",
        "      dissimilarity = feat.graycoprops(graycom, 'dissimilarity').reshape(-1)\n",
        "      homogeneity = feat.graycoprops(graycom, 'homogeneity').reshape(-1)\n",
        "      energy = feat.graycoprops(graycom, 'energy').reshape(-1)\n",
        "      correlation = feat.graycoprops(graycom, 'correlation').reshape(-1)\n",
        "      ASM = feat.graycoprops(graycom, 'ASM').reshape(-1)\n",
        "\n",
        "      # Append new cols to df\n",
        "      row_features = {\n",
        "          # info from preproc_map\n",
        "          'image_path': row['img_path'],\n",
        "          'image_source': row['image_name'],\n",
        "          'bbox_source': row['bbox_source'],\n",
        "          'class': row['class'],\n",
        "          'class_code':  row['class_code'],\n",
        "          'split': row['set'],\n",
        "          # moment features\n",
        "          'hu_moments': hu_mom,\n",
        "          'cm_rgb_mean': cm_rgb_mean,\n",
        "          'cm_rgb_var': cm_rgb_var,\n",
        "          'cm_rgb_skew': cm_rgb_skew,\n",
        "          'cm_hsv_mean': cm_hsv_mean,\n",
        "          'cm_hsv_var': cm_hsv_var,\n",
        "          'cm_hsv_skew': cm_hsv_skew,\n",
        "          'cm_ycrcb_mean': cm_ycrcb_mean,\n",
        "          'cm_ycrcb_var': cm_ycrcb_var,\n",
        "          'cm_ycrcb_skew': cm_ycrcb_skew,\n",
        "          # texture features\n",
        "          'contrast': contrast,\n",
        "          'dissimilarity': dissimilarity,\n",
        "          'homogeneity': homogeneity,\n",
        "          'energy': energy,\n",
        "          'correlation': correlation,\n",
        "          'ASM': ASM,\n",
        "          }\n",
        "      features = features.append(row_features, ignore_index=True)\n",
        "\n",
        "      if ((idx+1)%1000==0): # save every 1k records out\n",
        "        print(f\"\\nSaving df with {len(features)} records\")\n",
        "        features.to_pickle(drive_save_dir+'fe_subset_112722.csv')\n",
        "        features.to_pickle(bucket_save_dir+'fe_subset_112722.csv')\n",
        "\n",
        "    except:\n",
        "      skipped += 1\n",
        "      print(f\"Error #{skipped}\")\n",
        "      pass\n",
        "\n",
        "# Save final df\n",
        "features.to_pickle(drive_save_dir+'fe_subset_112722.csv')\n",
        "features.to_pickle(bucket_save_dir+'fe_subset_112722.csv')\n",
        "print(f\"Total FE errors: {skipped}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enlFvlEe1owN"
      },
      "source": [
        "## Merge in SIFT Histogram Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uysrT97KeMEM",
        "outputId": "349f6a99-a586-4a6d-f2c9-39fa5859562c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11584\n",
            "11584\n",
            "Null SIFT feature count: 138\n"
          ]
        }
      ],
      "source": [
        "# Load SIFT\n",
        "sift_train = pd.read_pickle(john_dir+'train_set.pkl')\n",
        "sift_val = pd.read_pickle(john_dir+'val_set.pkl')\n",
        "sift_test = pd.read_pickle(john_dir+'test_set.pkl')\n",
        "sift = pd.concat([sift_train, sift_val, sift_test], axis=0)\n",
        "\n",
        "# Merge\n",
        "sift['merge_key'] = sift['file'].apply(lambda x: '_'.join(x[:-4].split(\"_\")[:6]))\n",
        "features['merge_key'] = features['image_path'].apply(lambda x: '_'.join(x[:-4].split(\"_\")[:6]))\n",
        "print(len(features))\n",
        "features = pd.merge(features, sift[['merge_key', 'norm_hist']], on='merge_key', how='inner') # <-- should it be inner join?\n",
        "print(len(features))\n",
        "\n",
        "num_null = len(features.loc[features['norm_hist'].isnull(), ])\n",
        "print(f\"Null SIFT feature count: {num_null}\")\n",
        "\n",
        "# Save out merged features\n",
        "features.to_pickle(drive_save_dir+'fe_merged_all_112722.csv')\n",
        "#features.to_pickle(bucket_save_dir+'fe_merged_all_112722.csv')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "389GiZttb9NZ"
      },
      "source": [
        "## Save changes back to storage bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Xyg_u0dG22k6"
      },
      "outputs": [],
      "source": [
        "# after done working, copy files back to google bucket\n",
        "!gsutil -m -q cp -r /content/w281FinalProjectLogo/* gs://w281finalprojectlogo/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o8fM4fQ6kpRq"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.0 ('w281')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    },
    "vscode": {
      "interpreter": {
        "hash": "7690e95adedd8191f0ce86c5df02585ffc130729bec63a8b5bda66e6900601ad"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}