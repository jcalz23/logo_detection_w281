{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59914839",
   "metadata": {},
   "source": [
    "# Object classification model using SIFT features\n",
    "\n",
    "This notebook has two sections. Section 1 uses full images and extracts bounding boxes with manual coordinates and processes throug a SIFT histogram model. SVM and Logistic regression are used on train and validation sets for classfication. There is no data pre-processing or augmentation in this section. Section 2 uses already pre-processed and augmented bounding boxes as input data for training the SIFT model. This section also uses SVM and Logistic regression for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f2453b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install requied libraries\n",
    "!python -m pip install --upgrade pip\n",
    "!python -m pip install opencv-contrib-python --user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "57f2bd20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "import io\n",
    "import json\n",
    "import pandas as pd\n",
    "import pickle\n",
    "\n",
    "from PIL import Image\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from scipy.spatial import distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "976927a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vish/281/project'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set pwd\n",
    "PWD = !pwd\n",
    "PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d6ec490",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set base dir\n",
    "base_dir = PWD[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa45fdf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vish/281/project\r\n"
     ]
    }
   ],
   "source": [
    "!pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9a3b28fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/vish/281/project/gray/'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Set the directory where all the pre-processed and augmented data is present\n",
    "gray_dir = base_dir + \"/gray/\"\n",
    "gray_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "15877f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define paths for full images and yolo bb files\n",
    "image_dir = base_dir + \"data/all_images/\"\n",
    "bbox_dir = base_dir + \"data/yolofinalset/obj_train_data/\"\n",
    "\n",
    "#Change current directory\n",
    "os.chdir(base_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 793,
   "id": "3a215260",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['X_train', 'y_train', 'X_val', 'y_val', 'X_test', 'y_test'])"
      ]
     },
     "execution_count": 793,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load classes and input images from json files\n",
    "with open(base_dir+\"classes.json\") as infile:\n",
    "    classes = json.load(infile)\n",
    "\n",
    "with open(base_dir+\"train_test_split_img.json\") as infile:\n",
    "    inp_imgs = json.load(infile)\n",
    "\n",
    "# Check keys of train_test_split_img.json\n",
    "inp_imgs.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447126a4",
   "metadata": {},
   "source": [
    "### Section 1: Extract BB from images and no data augmentation or processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a16d80a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract a bounding box from the image \n",
    "# based on the yolo text file data\n",
    "\n",
    "def extract_bbox(img, center_x, center_y, width, height):\n",
    "    \n",
    "    img_y, img_x = img.shape\n",
    "    \n",
    "    min_x = round((center_x - 0.5*width) * img_x)\n",
    "    max_x = round((center_x + 0.5*width) * img_x)\n",
    "    min_y = round((center_y - 0.5*height) * img_y)\n",
    "    max_y = round((center_y + 0.5*height) * img_y)\n",
    "    \n",
    "    # return extracted bb\n",
    "    return img[min_y:max_y, min_x:max_x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 795,
   "id": "59a94f03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract all bounding boxes from a set of input images\n",
    "\n",
    "def extract_all_bb(input_imgs, set_tag):\n",
    "    \"\"\"parameters: \n",
    "        input_imgs: input list of image names\n",
    "        set_tag: \"train\" or \"val\" or \"test\"\n",
    "        output: total bb count for input list \n",
    "    \"\"\"\n",
    "    # Initialize bb count for input set\n",
    "    bb_cnt = 0\n",
    "\n",
    "    # for each image get the yolo bb text file\n",
    "    for image in input_imgs:\n",
    "        img_path = image_dir + image\n",
    "        bb_path = bbox_dir + image.replace('.jpg','.txt')\n",
    "\n",
    "\n",
    "        # Convert ref image to grayscale\n",
    "        img_color = plt.imread(img_path)\n",
    "        img_gray = cv2.cvtColor(img_color, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        # Open each yolo text and read all lines\n",
    "        # yolov5 format - class_id center_x center_y width height\n",
    "        with open(bb_path) as f:\n",
    "            input_bb = f.readlines()\n",
    "\n",
    "\n",
    "        # Assign values from yolo text file and extract bounding box\n",
    "        for bb in range(len(input_bb)):\n",
    "            cls, center_x, center_y, width, height = [float(x) for x in input_bb[bb].split()]\n",
    "            \n",
    "            #randomly generate bb variable name and assign extracted bb\n",
    "            globals()['bb_%s_%s' % (set_tag, bb_cnt)] = [extract_bbox(img_gray, center_x, center_y, width, height),cls]\n",
    "            \n",
    "            # increment bb count\n",
    "            bb_cnt += 1\n",
    "    \n",
    "    return bb_cnt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdee128d",
   "metadata": {},
   "source": [
    "### Data assignment\n",
    "* Get the list of tran, validation and test images from the input json\n",
    "* Extract and store all bbs from the different input list\n",
    "* The extrcted bbs and classes are stored in the global dictionary as a list [bb, cls]\n",
    "* Variable format is bb_x_y where x is \"train\" or \"val\" or \"test\" ad y is the bb number in the list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 796,
   "id": "ec5d78b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the list of images\n",
    "train_imgs = inp_imgs[\"X_train\"]\n",
    "val_imgs = inp_imgs[\"X_val\"]\n",
    "test_imgs = inp_imgs[\"X_test\"]\n",
    "\n",
    "# save bb and class for all sets\n",
    "train_bbs = extract_all_bb(train_imgs, \"train\")\n",
    "val_bbs = extract_all_bb(val_imgs, \"val\")\n",
    "test_bbs = extract_all_bb(test_imgs, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b218b67a",
   "metadata": {},
   "source": [
    "### Extract SIFT features from the training data and create histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 797,
   "id": "76c92afc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SIFT feature extractor\n",
    "sift = cv2.SIFT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "id": "5dcb4a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get keypoints and descripors for the given list of bbs\n",
    "\n",
    "def key_desc(bb_cnt, set_tag):\n",
    "    \"\"\"parameters: \n",
    "        bb_cnt: count of the input bbs\n",
    "        set_tag: \"train\" or \"val\" or \"test\" \n",
    "    \"\"\"\n",
    "    \n",
    "    # detect all keypoints, descriptors from all bbs\n",
    "    for bb in range(bb_cnt):\n",
    "        \n",
    "        # Assign the keypoints and descriptors to predefined global variables using set tag\n",
    "        globals()['kp_%s_%s' % (set_tag, bb)], globals()['dc_%s_%s' % (set_tag, bb)] \\\n",
    "                                = sift.detectAndCompute(globals()['bb_%s_%s' % (set_tag, bb)][0], None)\n",
    "        \n",
    "        # Assign classes to each bb to predefined global variables using set tag\n",
    "        globals()['cl_%s_%s' % (set_tag, bb)] = int(globals()['bb_%s_%s' % (set_tag, bb)][1])\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84debe37",
   "metadata": {},
   "source": [
    "Below we extract all keypoints and descriptors for the three different bb lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 799,
   "id": "2327db5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all keypoints and descriptors\n",
    "key_desc(train_bbs, \"train\")\n",
    "key_desc(val_bbs, \"val\")\n",
    "key_desc(test_bbs, \"test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3947b4f",
   "metadata": {},
   "source": [
    "Now that we have extracted all the keypoints and descriptors from the training bb list, we need to stack them up. This will help us run k-means on it and cluster them into different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 805,
   "id": "0755fd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to stack all keypoint descriptors in a set\n",
    "\n",
    "def dc_stack(bb_cnt, set_tag):\n",
    "    \"\"\"parameters: \n",
    "        bb_cnt: count of the input bbs\n",
    "        set_tag: \"train\" or \"val\" or \"test\" \n",
    "        output: stacked descriptors (Nx128)\n",
    "    \"\"\"\n",
    "        \n",
    "    #stack all keypoint descriptors for using k-means later\n",
    "    stack_dc = globals()['dc_%s_0' % (set_tag)]\n",
    "    \n",
    "    # Use try except for handling bbs with no keypoints or descriptors\n",
    "    for i in range((bb_cnt)-1):\n",
    "        try:\n",
    "            stack_dc = np.vstack((stack_dc,globals()['dc_%s_%s' % (set_tag, i+1)]))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return stack_dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 808,
   "id": "4638fea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(191977, 128)\n"
     ]
    }
   ],
   "source": [
    "# stack all training keypoint descriptors by calling the function\n",
    "train_dc = dc_stack(train_bbs, \"train\")\n",
    "\n",
    "# check shape of stacked descriptors\n",
    "print(train_dc.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26819985",
   "metadata": {},
   "source": [
    "### K-means clustering\n",
    "Run k-means algorithm on the extracted descriptors from the training set to form different clusters. Number of clusters is a hyper parameter arrived at after tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 809,
   "id": "3dc8f32b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# No of clusters/bins\n",
    "dim = 350\n",
    "\n",
    "# Instantiate k-means\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=dim, n_init=4)\n",
    "\n",
    "# Fit all training keypoints and transform\n",
    "dc_km = kmeans.fit_transform(train_dc)\n",
    "\n",
    "# Get cluster labels for the training keypoints\n",
    "dc_kmy = kmeans.fit_predict(train_dc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb90c0d1",
   "metadata": {},
   "source": [
    "**Data validation - Perform some checks**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 810,
   "id": "a4b77bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of keypoint descriptors after clustering:  (191977, 350)\n",
      "Shape of keypoint descriptor labels:  (191977,)\n",
      "Minimum cluster # 0 Maximum cluster # 349\n",
      "Cluster centers shape:  (350, 128)\n"
     ]
    }
   ],
   "source": [
    "# Checks shapes of the transformed data\n",
    "print(\"Shape of keypoint descriptors after clustering: \",dc_km.shape)\n",
    "print(\"Shape of keypoint descriptor labels: \",dc_kmy.shape)\n",
    "\n",
    "# min and max clusters\n",
    "print(\"Minimum cluster #\", min(dc_kmy), \"Maximum cluster #\", max(dc_kmy))\n",
    "\n",
    "# Cluster identification for each keypoint\n",
    "\n",
    "# Cluster centers shape\n",
    "print(\"Cluster centers shape: \", kmeans.cluster_centers_.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5e9de81",
   "metadata": {},
   "source": [
    "**Building SIFT histogram for each bb based on the keypoint descriptor clusters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 811,
   "id": "40a2e9f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build SIFT histogram from keypoint descriptors by grouping into clusters\n",
    "def build_hist(bb_cnt, set_tag, dim):\n",
    "    \"\"\"parameters: \n",
    "        bb_cnt: count of the input bbs\n",
    "        set_tag: \"train\" or \"val\" or \"test\"\n",
    "        dim: number of clusters used as bins\n",
    "    \"\"\"\n",
    "    # Use try except for handling bbs with no keypoints or descriptors\n",
    "    for bb in range(bb_cnt): \n",
    "        try:\n",
    "            # Run the bb keyponts through training kmeans \n",
    "            globals()['clst_%s_%s' % (set_tag, bb)] = kmeans.predict(globals()['dc_%s_%s' % (set_tag, bb)])\n",
    "        except:\n",
    "            # Set histogram to zero if there are no keypoints\n",
    "            globals()['his_%s_%s' % (set_tag, bb)] = (np.zeros((dim,),dtype=int),np.arange(0,dim+1))\n",
    "        else:\n",
    "            # Create histogram based on assigned clusters when descriptors are available\n",
    "            globals()['his_%s_%s' % (set_tag, bb)] = np.histogram(globals()['clst_%s_%s' % (set_tag, bb)], \\\n",
    "                                                                  bins=dim,range=(0,dim))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d5205f",
   "metadata": {},
   "source": [
    "**Building SIFT histograms for the training set**\n",
    "<br/>The histogram is stored in a global variable of format is his_x_y where x is \"train\" or \"val\" or \"test\" ad y is the bb number in the list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 812,
   "id": "cc5220a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SIFT histograms using clusters dimensions\n",
    "build_hist(train_bbs, \"train\", dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 813,
   "id": "ee512046",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 3, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0,\n",
       "        0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0, 1,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0,\n",
       "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
       "        0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 1, 0],\n",
       "       dtype=int64),\n",
       " array([  0.,   1.,   2.,   3.,   4.,   5.,   6.,   7.,   8.,   9.,  10.,\n",
       "         11.,  12.,  13.,  14.,  15.,  16.,  17.,  18.,  19.,  20.,  21.,\n",
       "         22.,  23.,  24.,  25.,  26.,  27.,  28.,  29.,  30.,  31.,  32.,\n",
       "         33.,  34.,  35.,  36.,  37.,  38.,  39.,  40.,  41.,  42.,  43.,\n",
       "         44.,  45.,  46.,  47.,  48.,  49.,  50.,  51.,  52.,  53.,  54.,\n",
       "         55.,  56.,  57.,  58.,  59.,  60.,  61.,  62.,  63.,  64.,  65.,\n",
       "         66.,  67.,  68.,  69.,  70.,  71.,  72.,  73.,  74.,  75.,  76.,\n",
       "         77.,  78.,  79.,  80.,  81.,  82.,  83.,  84.,  85.,  86.,  87.,\n",
       "         88.,  89.,  90.,  91.,  92.,  93.,  94.,  95.,  96.,  97.,  98.,\n",
       "         99., 100., 101., 102., 103., 104., 105., 106., 107., 108., 109.,\n",
       "        110., 111., 112., 113., 114., 115., 116., 117., 118., 119., 120.,\n",
       "        121., 122., 123., 124., 125., 126., 127., 128., 129., 130., 131.,\n",
       "        132., 133., 134., 135., 136., 137., 138., 139., 140., 141., 142.,\n",
       "        143., 144., 145., 146., 147., 148., 149., 150., 151., 152., 153.,\n",
       "        154., 155., 156., 157., 158., 159., 160., 161., 162., 163., 164.,\n",
       "        165., 166., 167., 168., 169., 170., 171., 172., 173., 174., 175.,\n",
       "        176., 177., 178., 179., 180., 181., 182., 183., 184., 185., 186.,\n",
       "        187., 188., 189., 190., 191., 192., 193., 194., 195., 196., 197.,\n",
       "        198., 199., 200., 201., 202., 203., 204., 205., 206., 207., 208.,\n",
       "        209., 210., 211., 212., 213., 214., 215., 216., 217., 218., 219.,\n",
       "        220., 221., 222., 223., 224., 225., 226., 227., 228., 229., 230.,\n",
       "        231., 232., 233., 234., 235., 236., 237., 238., 239., 240., 241.,\n",
       "        242., 243., 244., 245., 246., 247., 248., 249., 250., 251., 252.,\n",
       "        253., 254., 255., 256., 257., 258., 259., 260., 261., 262., 263.,\n",
       "        264., 265., 266., 267., 268., 269., 270., 271., 272., 273., 274.,\n",
       "        275., 276., 277., 278., 279., 280., 281., 282., 283., 284., 285.,\n",
       "        286., 287., 288., 289., 290., 291., 292., 293., 294., 295., 296.,\n",
       "        297., 298., 299., 300., 301., 302., 303., 304., 305., 306., 307.,\n",
       "        308., 309., 310., 311., 312., 313., 314., 315., 316., 317., 318.,\n",
       "        319., 320., 321., 322., 323., 324., 325., 326., 327., 328., 329.,\n",
       "        330., 331., 332., 333., 334., 335., 336., 337., 338., 339., 340.,\n",
       "        341., 342., 343., 344., 345., 346., 347., 348., 349., 350.]))"
      ]
     },
     "execution_count": 813,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check a value of the histogram and its bins\n",
    "his_train_187"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 814,
   "id": "44fbc994",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 814,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check value of the corresponding class label\n",
    "cl_train_187"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e31bb34",
   "metadata": {},
   "source": [
    "**Normalizing all the SIFT histograms by dividing with the max value**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 817,
   "id": "492f95e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize histogram function\n",
    "\n",
    "def hist_norm(bb_cnt, set_tag):\n",
    "    \"\"\"parameters: \n",
    "        bb_cnt: count of the input bbs\n",
    "        set_tag: \"train\" or \"val\" or \"test\"\n",
    "    \"\"\"\n",
    "    # for each bb, normalize the SIFT histogram by dividing the max value\n",
    "    # use if else to avoid dividing by 0\n",
    "    \n",
    "    for bb in range(bb_cnt):\n",
    "        \n",
    "        if np.max(globals()['his_%s_%s' % (set_tag, bb)][0]) == 0:\n",
    "            globals()['his_norm_%s_%s' % (set_tag, bb)] = \\\n",
    "              globals()['his_%s_%s' % (set_tag, bb)][0]\n",
    "        else:\n",
    "            globals()['his_norm_%s_%s' % (set_tag, bb)] = \\\n",
    "              globals()['his_%s_%s' % (set_tag, bb)][0]/np.max(globals()['his_%s_%s' % (set_tag, bb)][0])\n",
    "        \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 818,
   "id": "ad6f7f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalizing train histograms\n",
    "hist_norm(train_bbs, \"train\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89016576",
   "metadata": {},
   "source": [
    "### Creating classification models\n",
    "* Use the normalized SIFT histograms to train different linearly seperable (SVM and logistic regression) and simple L2 distance models\n",
    "* Here we are going to use a One vs Rest classification model i.e. each class will be predicted with a yes or no when compared with all other classes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f400c8",
   "metadata": {},
   "source": [
    "#### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "id": "5ae49029",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the normalized histograms and label data for the train set \n",
    "xtrain = [globals()['his_norm_train_%s' % bb] for bb in range(train_bbs)]\n",
    "ytrain = [globals()['cl_train_%s' % bb] for bb in range(train_bbs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 820,
   "id": "919d0ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate linear SVC and logistic regression models\n",
    "lsvc = LinearSVC(penalty='l2', dual=False, multi_class = 'ovr', max_iter = 5000, C = 0.22, class_weight='balanced')\n",
    "logovr = OneVsRestClassifier(LogisticRegression())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 821,
   "id": "4746b227",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC best score on Train set: 0.9311\n"
     ]
    }
   ],
   "source": [
    "# Fit the training histograms and predict with SVC on training data itself\n",
    "lsvc.fit(xtrain, ytrain)\n",
    "\n",
    "y_train_score = lsvc.score(xtrain, ytrain)\n",
    "\n",
    "print(\"SVC best score on Train set: {:.4f}\".format(y_train_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 828,
   "id": "5387d779",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Classifier best score on Train set: 0.9038\n"
     ]
    }
   ],
   "source": [
    "# Fit the training histograms and predict with Logistic regression on training data itself \n",
    "logovr.fit(xtrain, ytrain)\n",
    "\n",
    "y_train_pred = logovr.predict(xtrain)\n",
    "\n",
    "print('LR Classifier best score on Train set: {:.4f}'.format(accuracy_score(ytrain, y_train_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e73166",
   "metadata": {},
   "source": [
    "#### Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "945b6aa8",
   "metadata": {},
   "source": [
    "Check accuracy on validation set. Parameters were tuned using validation set performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "id": "50ed3f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to build SIFT histogram from keypoint descriptors by grouping into clusters\n",
    "def build_hist_val(desc, set_tag, dim, bb):\n",
    "    \"\"\"parameters:\n",
    "        desc: keypoint descriptors variable for the input bb \n",
    "        set_tag: \"train\" or \"val\" or \"test\"\n",
    "        dim: number of clusters used as bins\n",
    "        bb: the index number of the bb from the list for which histogram is requested\n",
    "    \"\"\"\n",
    "    \n",
    "    # Use try except for handling bbs with no keypoints or descriptors\n",
    "    try:\n",
    "        # Run the bb keyponts through trained kmeans and assign clusters\n",
    "        globals()['clst_%s_%s' % (set_tag, bb)] = kmeans.predict(desc)\n",
    "    except:\n",
    "        # Set histogram to zero if there are no keypoints\n",
    "        globals()['his_%s_%s' % (set_tag, bb)] = (np.zeros((dim,),dtype=int),np.arange(0,dim+1))\n",
    "    else:\n",
    "        # Create histogram based on assigned clusters when descriptors are available for the bb\n",
    "        globals()['his_%s_%s' % (set_tag, bb)] = np.histogram(globals()['clst_%s_%s' % (set_tag, bb)], \\\n",
    "                                                              bins=dim,range=(0,dim))\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "id": "8b575ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build SIFT histogram for keypoints in each bb in the validation list\n",
    "for bb in range(val_bbs):\n",
    "    build_hist_val(globals()['dc_%s_%s' % (\"val\", bb)], \"val\", dim, bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 825,
   "id": "9aa05ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize SIFT histograms from the validation bb list\n",
    "for bb in range(val_bbs):\n",
    "    \n",
    "    # for each bb, normalize the SIFT histogram by dividing the max value\n",
    "    # use if else to avoid dividing by 0\n",
    "        \n",
    "    if np.max(globals()['his_%s_%s' % (\"val\", bb)][0]) == 0:\n",
    "            globals()['his_norm_%s_%s' % (\"val\", bb)] = \\\n",
    "              globals()['his_%s_%s' % (\"val\", bb)][0]\n",
    "    else:\n",
    "            globals()['his_norm_%s_%s' % (\"val\", bb)] = \\\n",
    "              globals()['his_%s_%s' % (\"val\", bb)][0]/np.max(globals()['his_%s_%s' % (\"val\", bb)][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "id": "748e85d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the normalized histograms and label data for the validation set \n",
    "xval = [globals()['his_norm_val_%s' % bb] for bb in range(val_bbs)]\n",
    "yval = [globals()['cl_val_%s' % bb] for bb in range(val_bbs)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 830,
   "id": "332f15e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC best score on Val set: 0.7812\n"
     ]
    }
   ],
   "source": [
    "# Predict with SVC on validation data\n",
    "y_val_pred = lsvc.predict(xval)\n",
    "\n",
    "print('SVC best score on Val set: {:.4f}'.format(accuracy_score(yval, y_val_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 831,
   "id": "194ebc96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Classifier best score on Val set: 0.75\n"
     ]
    }
   ],
   "source": [
    "# Predict with SVC on validation data\n",
    "\n",
    "y_val_pred = logovr.predict(xval)\n",
    "\n",
    "print('LR Classifier best score on Val set: {:.2f}'.format(accuracy_score(yval, y_val_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62caf312",
   "metadata": {},
   "source": [
    "Creating a third model to predict the class of the training SIFT histogram with the least L2 distance from validation sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "id": "ea79d754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Manual L2 distance check of SIFT histograms (Val vs Train)\n",
    "\n",
    "# Create an empty list to store predictions\n",
    "val_l2 = list()\n",
    "\n",
    "\n",
    "# For each validation histogram check Euclidean distace with all training histograms and pick the least\n",
    "for val in range(val_bbs):\n",
    "    \n",
    "    # Initialize the least distance and class label with first traing data\n",
    "    lst_dist = distance.euclidean(xval[val],xtrain[0])\n",
    "    cur_cls = yval[0]\n",
    "    \n",
    "    # Check the current validation histogram with every training histogram\n",
    "    for train in range(train_bbs):\n",
    "        \n",
    "        # Calculate Euclidean distance\n",
    "        cur_dist = distance.euclidean(xval[val],xtrain[train])\n",
    "        \n",
    "        # If the current distance is lower than the least one update least distance and current class\n",
    "        if cur_dist < lst_dist:\n",
    "            lst_dist = cur_dist\n",
    "            cur_cls = ytrain[train]\n",
    "    \n",
    "    # Append the final class to prediction list \n",
    "    val_l2.append(cur_cls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 833,
   "id": "90e22ede",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "L2 dist Classifier best score on Val set: 0.5596\n"
     ]
    }
   ],
   "source": [
    "# Prediction score with manual L2 distance model on validation data\n",
    "\n",
    "print('L2 dist Classifier best score on Val set: {:.4f}'.format(accuracy_score(yval, val_l2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c36959",
   "metadata": {},
   "source": [
    "### Section 2: Process using preprocessed and augmented data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "c3686de1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load classes and augmented gray input images from json files\n",
    "with open(base_dir+\"/preproc_map_gray.json\") as infile:\n",
    "    gray_imgs = json.load(infile)\n",
    "\n",
    "with open(base_dir+\"/class_map_summary_gray.json\") as infile:\n",
    "    cls_summary = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "496ce611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('000398_01_bb_cn_00_da_gray.jpg', {'image_source': '/images/apple/000398.jpg', 'bbox_source': '/yolofinalset/obj_train_data/000398.txt', 'preprocessing': ['bbox', 'cn', 'da', 'gray'], 'class': 'adidas', 'class_code': 0, 'set': 'train'})\n",
      "\n",
      " ('adidas', {'code': 0, 'num_bbs': 1528})\n"
     ]
    }
   ],
   "source": [
    "# Check a key and value\n",
    "print(list(gray_imgs.items())[0])\n",
    "\n",
    "print(\"\\n\",list(cls_summary.items())[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9d0e1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataframe from class summary json\n",
    "cls_summary = pd.DataFrame.from_dict(cls_summary)\n",
    "cls_summary = cls_summary.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "207972df",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>num_bbs</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>adidas</th>\n",
       "      <td>0</td>\n",
       "      <td>1528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dhl</th>\n",
       "      <td>1</td>\n",
       "      <td>1528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apple</th>\n",
       "      <td>2</td>\n",
       "      <td>1528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bmw</th>\n",
       "      <td>3</td>\n",
       "      <td>1528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cocacola</th>\n",
       "      <td>4</td>\n",
       "      <td>1528</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          code  num_bbs\n",
       "adidas       0     1528\n",
       "dhl          1     1528\n",
       "apple        2     1528\n",
       "bmw          3     1528\n",
       "cocacola     4     1528"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_summary.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "036dcaa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a dataframe from gray pre-processed and augmented images json\n",
    "gray_imgs = pd.DataFrame.from_dict(gray_imgs)\n",
    "gray_imgs = gray_imgs.T"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa96ebe",
   "metadata": {},
   "source": [
    "### Data assignment\n",
    "* Get the list of tran, validation and test images from the dataframe created from input json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "7108b20c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the train, validation and test file names based on the set name in the dataframe\n",
    "train_set = gray_imgs[gray_imgs[\"set\"]==\"train\"]\n",
    "train_set = train_set.rename_axis('file').reset_index(level=0)\n",
    "train_set = train_set.rename_axis('id').reset_index(level=0)\n",
    "\n",
    "val_set = gray_imgs[gray_imgs[\"set\"]==\"val\"]\n",
    "val_set = val_set.rename_axis('file').reset_index(level=0)\n",
    "val_set = val_set.rename_axis('id').reset_index(level=0)\n",
    "\n",
    "test_set = gray_imgs[gray_imgs[\"set\"]==\"test\"]\n",
    "test_set = test_set.rename_axis('file').reset_index(level=0)\n",
    "test_set = test_set.rename_axis('id').reset_index(level=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "6e73ebc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>file</th>\n",
       "      <th>image_source</th>\n",
       "      <th>bbox_source</th>\n",
       "      <th>preprocessing</th>\n",
       "      <th>class</th>\n",
       "      <th>class_code</th>\n",
       "      <th>set</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>000398_01_bb_cn_00_da_gray.jpg</td>\n",
       "      <td>/images/apple/000398.jpg</td>\n",
       "      <td>/yolofinalset/obj_train_data/000398.txt</td>\n",
       "      <td>[bbox, cn, da, gray]</td>\n",
       "      <td>adidas</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>000398_01_bb_cn_01_da_gray.jpg</td>\n",
       "      <td>/images/apple/000398.jpg</td>\n",
       "      <td>/yolofinalset/obj_train_data/000398.txt</td>\n",
       "      <td>[bbox, cn, da, gray]</td>\n",
       "      <td>adidas</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>000398_01_bb_cn_02_da_gray.jpg</td>\n",
       "      <td>/images/apple/000398.jpg</td>\n",
       "      <td>/yolofinalset/obj_train_data/000398.txt</td>\n",
       "      <td>[bbox, cn, da, gray]</td>\n",
       "      <td>adidas</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>000398_01_bb_cn_03_da_gray.jpg</td>\n",
       "      <td>/images/apple/000398.jpg</td>\n",
       "      <td>/yolofinalset/obj_train_data/000398.txt</td>\n",
       "      <td>[bbox, cn, da, gray]</td>\n",
       "      <td>adidas</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>000398_01_bb_cn_04_da_gray.jpg</td>\n",
       "      <td>/images/apple/000398.jpg</td>\n",
       "      <td>/yolofinalset/obj_train_data/000398.txt</td>\n",
       "      <td>[bbox, cn, da, gray]</td>\n",
       "      <td>adidas</td>\n",
       "      <td>0</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                            file              image_source  \\\n",
       "0   0  000398_01_bb_cn_00_da_gray.jpg  /images/apple/000398.jpg   \n",
       "1   1  000398_01_bb_cn_01_da_gray.jpg  /images/apple/000398.jpg   \n",
       "2   2  000398_01_bb_cn_02_da_gray.jpg  /images/apple/000398.jpg   \n",
       "3   3  000398_01_bb_cn_03_da_gray.jpg  /images/apple/000398.jpg   \n",
       "4   4  000398_01_bb_cn_04_da_gray.jpg  /images/apple/000398.jpg   \n",
       "\n",
       "                               bbox_source         preprocessing   class  \\\n",
       "0  /yolofinalset/obj_train_data/000398.txt  [bbox, cn, da, gray]  adidas   \n",
       "1  /yolofinalset/obj_train_data/000398.txt  [bbox, cn, da, gray]  adidas   \n",
       "2  /yolofinalset/obj_train_data/000398.txt  [bbox, cn, da, gray]  adidas   \n",
       "3  /yolofinalset/obj_train_data/000398.txt  [bbox, cn, da, gray]  adidas   \n",
       "4  /yolofinalset/obj_train_data/000398.txt  [bbox, cn, da, gray]  adidas   \n",
       "\n",
       "  class_code    set  \n",
       "0          0  train  \n",
       "1          0  train  \n",
       "2          0  train  \n",
       "3          0  train  \n",
       "4          0  train  "
      ]
     },
     "execution_count": 278,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check a few rows in the train dataframe\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a975849",
   "metadata": {},
   "source": [
    "### Extract SIFT features from the training data and create histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "f141d0f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create SIFT feature extractor\n",
    "sift_bb = cv2.SIFT_create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "06321a9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get keypoints and descripors for the given list of bbs\n",
    "def key_desc_bb(file):\n",
    "    \"\"\"parameters: \n",
    "        file: input bb file name\n",
    "        output: returns descriptors of keypoints\n",
    "    \"\"\"\n",
    "    img_gray = plt.imread(gray_dir + file)\n",
    "    \n",
    "    kp, dc = sift_bb.detectAndCompute(img_gray, None)\n",
    "    \n",
    "    return dc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "330187fc",
   "metadata": {},
   "source": [
    "Below we extract all keypoints and descriptors for the three different bb lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "5fdfe537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to train dataframe and store all the keypoint descriptors for the corresponding files\n",
    "train_set['kp_dc'] = train_set['file'].apply(key_desc_bb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "4f508855",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the dataframe to pickle object for suture use\n",
    "pkl_obj = open('train_set.pkl', 'wb')\n",
    "pickle.dump(train_set,pkl_obj)\n",
    "pkl_obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19c96aa5",
   "metadata": {},
   "source": [
    "Now that we have extracted all the keypoints and descriptors from the training list, we need to stack them up. This will help us run k-means on it and cluster them into different groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "edf4cc44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack all keypoint descriptors from all data skipping BBs with no keypoints\n",
    "# This is to build clusters (BOW) and to build SIFT histogram\n",
    "stack_desc = np.vstack(train_set[train_set['kp_dc'].apply(lambda x: x is not None)]['kp_dc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "f6aaac53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1514473, 128)"
      ]
     },
     "execution_count": 285,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of the stacked descriptors\n",
    "stack_desc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "205cb1a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(730, 128)"
      ]
     },
     "execution_count": 286,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check shape of the descriptors of one of the files\n",
    "train_set['kp_dc'][1100].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14270699",
   "metadata": {},
   "source": [
    "### K-means clustering\n",
    "Run k-means algorithm on the extracted descriptors from the training set to form different clusters. Number of clusters is a hyper parameter arrived at after tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "a15e441f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define number of clusters/bins\n",
    "dim = 1500\n",
    "\n",
    "# Instantiate k-means\n",
    "kmeans = KMeans(init=\"k-means++\", n_clusters=dim, n_init=3)\n",
    "\n",
    "# Fit all training keypoints and transform\n",
    "dc_km = kmeans.fit_transform(stack_desc)\n",
    "\n",
    "# Get cluster labels for the training keypoints\n",
    "dc_kmy = kmeans.fit_predict(stack_desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "ba9bc6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1514473, 1500)"
      ]
     },
     "execution_count": 288,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the descriptors after kmeans\n",
    "# Number of features will be based on the bins\n",
    "dc_km.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "90874625",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1514473,)"
      ]
     },
     "execution_count": 289,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the shape of the cluster labels for the training keypoint descriptors\n",
    "dc_kmy.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0d632a",
   "metadata": {},
   "source": [
    "Building SIFT histogram for each bb based on the keypoint descriptor clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "1996aac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column and store the created SIFT histogram\n",
    "train_set[\"sift_hist\"] = train_set[train_set['kp_dc'].apply(lambda x: x is not None)]['kp_dc']\\\n",
    "                        .apply(lambda x: kmeans.predict(x))\\\n",
    "                            .apply(lambda x: np.histogram(x,bins = 1500, range = (0,1500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "c326d77f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the updated dataframe with SIFT histogram as a pickle object\n",
    "pkl_obj = open('train_set.pkl', 'wb')\n",
    "pickle.dump(train_set,pkl_obj)\n",
    "pkl_obj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a557980f",
   "metadata": {},
   "source": [
    "Normalize the SIFT histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "e2975719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform normalization on the SIFT histograms and add it to a new column\n",
    "train_set[\"norm_hist\"] = train_set[train_set['sift_hist'].apply(lambda x: type(x) is tuple)]['sift_hist']\\\n",
    "                        .apply(lambda x: x[0]/np.max(x[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f212cf7",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "538c8c98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the normalized histograms and label data for the train set \n",
    "xtrain = list(train_set[train_set['sift_hist'].apply(lambda x: type(x) is tuple)]['norm_hist'])\n",
    "ytrain = list(train_set[train_set['sift_hist'].apply(lambda x: type(x) is tuple)]['class_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dce8725",
   "metadata": {},
   "outputs": [],
   "source": [
    "#xtrain[xtrain.apply(lambda x: np.isnan(np.isnan(np.sum(x))))]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1c31816",
   "metadata": {},
   "source": [
    "Train a SVC and a Logistic regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "468c5812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate linear SVC and logistic regression models\n",
    "lsvc = LinearSVC(penalty='l2', dual=False, multi_class = 'ovr', max_iter = 5000, C = 0.01, class_weight='balanced')\n",
    "logovr = OneVsRestClassifier(LogisticRegression(penalty='l2', multi_class = 'ovr',\\\n",
    "                                                solver='liblinear',C = 0.1, max_iter = 5000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "aa8a33ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC best score on Train set: 0.8883\n"
     ]
    }
   ],
   "source": [
    "# Fit the training histograms and predict with SVC on training data itself\n",
    "lsvc.fit(xtrain, ytrain)\n",
    "\n",
    "y_train_score = lsvc.score(xtrain, ytrain)\n",
    "\n",
    "print(\"SVC best score on Train set: {:.4f}\".format(y_train_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "3746d5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Classifier best score on Train set: 0.8839\n"
     ]
    }
   ],
   "source": [
    "# Fit the training histograms and predict with Logistic regression on training data itself \n",
    "logovr.fit(xtrain, ytrain)\n",
    "\n",
    "y_train_pred = logovr.predict(xtrain)\n",
    "\n",
    "print('LR Classifier best score on Train set: {:.4f}'.format(accuracy_score(ytrain, y_train_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "99901d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load classes and augmented gray input images from json files\n",
    "# with open(base_dir+\"/preproc_map_cn.json\") as infile:\n",
    "#     cn_imgs = json.load(infile)\n",
    "\n",
    "# with open(base_dir+\"/class_map_summary_cn.json\") as infile:\n",
    "#     cn_cls_summary = json.load(infile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "14194dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cn_cls_summary = pd.DataFrame.from_dict(cn_cls_summary)\n",
    "# cn_cls_summary = cn_cls_summary.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "861f6383",
   "metadata": {},
   "outputs": [],
   "source": [
    "# cn_imgs = pd.DataFrame.from_dict(cn_imgs)\n",
    "# cn_imgs = cn_imgs.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "3945a3b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# val_set = cn_imgs[cn_imgs[\"set\"]==\"val\"]\n",
    "# val_set = val_set.rename_axis('file').reset_index(level=0)\n",
    "# val_set = val_set.rename_axis('id').reset_index(level=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bda757",
   "metadata": {},
   "source": [
    "Perform the same set of steps on the validation files to extract keypoint descriptors, predict using the prevously trained kmeans model, build SIFT histogram, normalized and then predict using the previously trained SVC and Logistic Regression models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "66b6c584",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get keypoints and descripors for the given list of bbs\n",
    "\n",
    "def key_desc_bb_cn(file):\n",
    "    \"\"\"parameters: \n",
    "        file: input bb file name \n",
    "        output: returns keypoint descriptors\n",
    "    \"\"\"\n",
    "    img_clr = plt.imread(base_dir + '/cn/' + file)\n",
    "    gray = cv2.cvtColor(img_clr, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    kp, dc = sift_bb.detectAndCompute(gray, None)\n",
    "    \n",
    "    return dc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "bb13ec15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a column to val dataframe and store all the keypoint descriptors for the corresponding files\n",
    "val_set['kp_dc'] = val_set['file'].apply(key_desc_bb_cn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "abcbda02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict using the prevously trained kmeans model and assign clusters\n",
    "val_set['kp_dc_cls'] = val_set[val_set['kp_dc'].apply(lambda x: x is not None)]['kp_dc']\\\n",
    "                            .apply(lambda x : kmeans.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "9fa960f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column and store the created SIFT histogram\n",
    "val_set[\"sift_hist\"] = val_set[val_set['kp_dc'].apply(lambda x: x is not None)]['kp_dc_cls']\\\n",
    "                        .apply(lambda x: np.histogram(x,bins = 1500, range = (0,1500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "9ef06f06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform normalization on the SIFT histograms and add it to a new column\n",
    "val_set[\"norm_hist\"] = val_set[val_set['sift_hist'].apply(lambda x: type(x) is tuple)]['sift_hist']\\\n",
    "                        .apply(lambda x: x[0]/np.max(x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "bf43ec5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the normalized histograms and label data for the val set \n",
    "xval = list(val_set[val_set['sift_hist'].apply(lambda x: type(x) is tuple)]['norm_hist'])\n",
    "yval = list(val_set[val_set['sift_hist'].apply(lambda x: type(x) is tuple)]['class_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "4b87ad20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVC best score on Train set: 0.7833\n"
     ]
    }
   ],
   "source": [
    "# Predict val set using previously trained SVC model\n",
    "\n",
    "y_val_score = lsvc.score(xval, yval)\n",
    "\n",
    "print(\"SVC best score on Train set: {:.4f}\".format(y_val_score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "548205d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR Classifier best score on Train set: 0.7917\n"
     ]
    }
   ],
   "source": [
    "# Predict val set using previously trained Logistic Regression model\n",
    "\n",
    "y_val_pred = logovr.predict(xval)\n",
    "\n",
    "print('LR Classifier best score on Train set: {:.4f}'.format(accuracy_score(yval, y_val_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f27eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the validation dataframe\n",
    "pkl_obj = open('val_set.pkl', 'wb')\n",
    "pickle.dump(val_set,pkl_obj)\n",
    "pkl_obj.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb87a683",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2013482c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f59d19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf3066f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5803e025",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
